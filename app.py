import os
import streamlit as st
from rag import get_relevant_context_from_db, generate_rag_prompt, answer

# Charger la cl√© API Gemini
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Si ce n'est pas d√©j√† dans l'√©tat de la session, initialise les r√©ponses, requ√™tes et historique
if 'history' not in st.session_state:
    st.session_state.history = []

# D√©finir l'interface de l'application Streamlit
st.title("Healthy Cooking AI Chatbot")
st.subheader("What do you want?")

# Conteneur pour afficher la conversation
response_container = st.container()

# Zone de saisie de la question
prompt = st.chat_input("Your question:")

if prompt:
    # Ajouter la question de l'utilisateur √† l'historique
    st.session_state.history.append({
        'role': 'user',
        'content': prompt
    })

    # Afficher le message de l'utilisateur
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner('üí° Typing...'):
        # R√©cup√©rer le contexte pertinent depuis la base de donn√©es
        context = get_relevant_context_from_db(prompt)

        # G√©n√©rer le prompt avec la question, le contexte et l'historique
        generated_prompt = generate_rag_prompt(query=prompt, context=context, history=[msg['content'] for msg in st.session_state.history])

        # Appeler la fonction pour obtenir la r√©ponse via l'API Gemini
        answer_text = answer(generated_prompt)  # Utilisation de la fonction answer

        # Ajouter la r√©ponse de l'assistant √† l'historique
        st.session_state.history.append({
            'role': 'assistant',
            'content': answer_text
        })

    # Afficher l'historique de la conversation
    with response_container:
        for msg in st.session_state.history:
            with st.chat_message(msg['role']):
                st.markdown(msg['content'])
